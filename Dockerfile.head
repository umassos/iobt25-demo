# Dockerfile for Jetson Orin Nano - Head Server
# Based on NVIDIA L4T (Linux for Tegra) runtime

FROM nvcr.io/nvidia/l4t-pytorch:r35.2.1-pth2.0-py3
LABEL authors="Krishna Praneet Gudipaty"
LABEL service="head-server"
LABEL description="Head server for IOBT25 ensemble inference"

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    libgomp1 \
    libgcc-s1 \
    libstdc++6 \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python3 -m pip install --upgrade pip

# Install ONNX Runtime for Jetson
# Note: Using CPU version as base, CUDA support will be enabled via environment
RUN pip3 install onnxruntime==1.16.3

# Install other Python dependencies
RUN pip3 install \
    grpcio==1.59.3 \
    grpcio-tools==1.59.3 \
    protobuf==4.25.1 \
    numpy==1.24.3 \
    onnx==1.15.0

# Create app directory
WORKDIR /app

# Declare models directory as a volume for mounting
VOLUME ["/app/models"]

# Copy requirements (for reference, but we install specific versions above)
COPY requirements.txt ./

# Expose the default port
EXPOSE 8185

# # Set the default command
# ENTRYPOINT ["python3", "system/head_server.py"]

# # Default arguments for head server
# CMD ["-m", "ensemble-effnet-c5-lr-0.005-tin", "-p", "8185"]
