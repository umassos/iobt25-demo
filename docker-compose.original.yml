version: '3.8'

services:
  original-server:
    build:
      context: .
      dockerfile: ${DOCKERFILE_ORIGINAL:-Dockerfile.original}
      tags: ["${IMAGE_TAG_ORIGINAL:-original-server:latest}"]
    container_name: ${CONTAINER_NAME_ORIGINAL:-original-server}
    ports:
      - "${ORIGINAL_SERVER_PORT:-8183}:8180"
    volumes:
      # Mount your Python source code for development (comment out for production)
      - ${SOURCE_CODE_PATH:-./system}:/app/system
      # Mount your ONNX models directory
      - ${MODELS_PATH:-./models}:/app/models:ro
      # Mount logs directory (optional)
      - ${LOGS_PATH:-./logs}:/app/logs
      - ./3rdparty:/app/3rdparty
      - ~/.cache/torch/hub/checkpoints:/root/.cache/torch/hub/checkpoints
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - PYTHONUNBUFFERED=${PYTHON_UNBUFFERED:-1}
    restart: ${RESTART_POLICY:-no}
    # GPU runtime requirements
    runtime: ${DOCKER_RUNTIME:-nvidia}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    # Override default command if needed
    entrypoint: ["python3", "system/single_server.py"]
    command: ["-m", "${ORIGINAL_SERVER_MODEL_NAME:-ensemble-effnet-c5-lr-0.005-tin}", "-n", "${ORIGINAL_SERVER_MODEL_PART:-1}", "-p", "8180", "-s", "${HEAD_SERVER_ADDR:-localhost:8185}"] 